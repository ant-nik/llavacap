{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0j5jcx0VifEZQ272XABwr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation models for zero-shot detection and segmentation\n",
        "\n",
        "Based on [Ollama](https://github.com/ollama/ollama) project."
      ],
      "metadata": {
        "id": "hc5E2kAe2Gxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Python tools"
      ],
      "metadata": {
        "id": "0AFbNeVJSK8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "vZQKC1XbhwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "SDwmJcs2BIat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import torch\n",
        "import PIL\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "import cv2\n",
        "import supervision\n",
        "import numpy\n",
        "import datetime\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "__detection_models = {}\n",
        "\n",
        "\n",
        "def get_model(name: str):\n",
        "    if not name in __detection_models:\n",
        "        __detection_models[name] = ZShotModel(model=name)\n",
        "    return __detection_models[name]\n",
        "\n",
        "\n",
        "class ZShotModel:\n",
        "    def __init__(self, model: str):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model=AutoModelForZeroShotObjectDetection.from_pretrained(\n",
        "            model).to(self.device)\n",
        "        self.processor=AutoProcessor.from_pretrained(model)\n",
        "\n",
        "    def infer(self, images: PIL.Image.Image | list[PIL.Image.Image],\n",
        "              prompt: str | list[str],\n",
        "              box_threshold: float=0.2,\n",
        "              text_threshold: float=0.2):\n",
        "        # VERY important: text queries need to be lowercased + end with a dot\n",
        "        if isinstance(prompt, list):\n",
        "            objects = [item for item in prompt if not item=='']\n",
        "            text = \" . \".join([f\"{item}\" for item in objects]).lower() + \".\"\n",
        "        elif isinstance(prompt, str):\n",
        "            text = prompt.lower() + \".\"\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Error, prompt with type \\\"{type(prompt)}\\\" is not supported\")\n",
        "        inputs = self.processor(\n",
        "            images=images, text=text, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        results = self.processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            target_sizes=[image.size[::-1]]\n",
        "        )\n",
        "\n",
        "        labels = results[0][\"labels\"]\n",
        "        class2label = list(set(labels))\n",
        "        label2class = {\n",
        "            class2label[i]: i for i in range(0, len(class2label))\n",
        "        }\n",
        "        classes = [label2class[label] for label in labels]\n",
        "        results[0][\"class2label\"] = class2label\n",
        "        results[0][\"label2class\"] = label2class\n",
        "        results[0][\"label_classes\"] = classes\n",
        "\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "def load_answer(filename: str) -> dict[str, any]:\n",
        "    with open(filename, \"r\") as file:\n",
        "        step2_response = json.loads(file.read())\n",
        "    return step2_response[\"response\"] if \"response\" in step2_response else step2_response\n",
        "\n",
        "\n",
        "def llm_infer(model: str,\n",
        "              prompt: str,\n",
        "              image_file: str | list[str] | None=None,\n",
        "              log_pattern: str | None=None,\n",
        "              url: str = \"http://localhost:11434/api/generate\"\n",
        "              ) -> dict[str, any]:\n",
        "    payload = dict(model=model, prompt=prompt, stream=False)\n",
        "    if image_file is not None:\n",
        "        if isinstance(image_file, str):\n",
        "            image_file = [image_file]\n",
        "        images = []\n",
        "        for image in image_file:\n",
        "            with open(image, \"rb\") as file:\n",
        "                encoded_image = base64.b64encode(file.read()).decode(\"ascii\")\n",
        "            images.append(encoded_image)\n",
        "        payload[\"images\"] = images\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_request.json\", \"w\") as file:\n",
        "            file.write(json.dumps(payload))\n",
        "\n",
        "    reply = requests.post(url, json=payload)\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_reply.json\", \"w\") as file:\n",
        "            file.write(reply.content.decode(\"ascii\"))\n",
        "\n",
        "\n",
        "    return json.loads(reply.content.decode(\"ascii\"))\n",
        "\n",
        "\n",
        "def text_to_objects(\n",
        "        text: str,\n",
        "        start: str=\"OUTPUT\",\n",
        "        end: str=\"INFO\",\n",
        "        rem: str=\":*\") -> list[str]:\n",
        "    clean_list = text\n",
        "    for char in rem:\n",
        "        clean_list = clean_list.replace(char, '')\n",
        "\n",
        "    split1 = clean_list.split(start)\n",
        "    if len(split1) < 2:\n",
        "        return []\n",
        "    split2 = split1[1].split(end)\n",
        "    if len(split2) < 2:\n",
        "        return []\n",
        "    objects = list(set(split2[0].split(\"\\n\")))\n",
        "    if '' in objects:\n",
        "        objects.remove('')\n",
        "\n",
        "    return objects\n",
        "\n",
        "\n",
        "def bbox_image(image: PIL.Image.Image, bbox: dict[str, any]) -> None:\n",
        "    box_annotator = supervision.BoxAnnotator()\n",
        "    label_annotator = supervision.LabelAnnotator()\n",
        "\n",
        "    image_boxes = supervision.Detections(\n",
        "        xyxy=bbox[\"boxes\"].cpu().numpy(),\n",
        "        class_id=numpy.array(bbox[\"label_classes\"], dtype=int)\n",
        "    )\n",
        "\n",
        "    #, 2, 3, 4])#results[0][\"labels\"]\n",
        "    \"\"\"\n",
        "    labels = [\n",
        "        f\"{class_id} {confidence:0.2f}\"\n",
        "        for confidence, class_id, boxes in results\n",
        "    ]\n",
        "    \"\"\"\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=image.copy(), detections=image_boxes) #, labels=labels)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=image_boxes,\n",
        "        labels=bbox[\"labels\"]\n",
        "    )\n",
        "    supervision.plot_image(annotated_frame, (16, 16))\n",
        "\n",
        "\n",
        "def detect_all_objects(\n",
        "        image: str,\n",
        "        captioning_prompt: str,\n",
        "        object_extraction_prompt: str,\n",
        "        bbox_threshold: float=0.1,\n",
        "        text_threshold: float=0.1,\n",
        "        captioning_model: str=\"llava\",\n",
        "        parsing_model: str=\"llama3.1\",\n",
        "        detection_model: str=\"IDEA-Research/grounding-dino-base\",\n",
        "        start_objects_list_marker: str=\"OUTPUT\",\n",
        "        end_objects_list_marker: str=\"INFO\",\n",
        "        symbols_to_remove: str=\"*:\",\n",
        "        capturing_iterations: int=1,\n",
        "        parsing_iterations: int=1\n",
        ") -> dict[str, any]:\n",
        "    calculation_id = (\n",
        "        captioning_model + \"_\" + datetime.datetime.utcnow().strftime(\n",
        "            \"%Y-%m-%d_%H.%M.%S_%f\")\n",
        "    )\n",
        "\n",
        "    llava_reply = []\n",
        "    for i in range(0, capturing_iterations):\n",
        "        try:\n",
        "            llava_reply.append(llm_infer(\n",
        "                    model=captioning_model,\n",
        "                    prompt=captioning_prompt,\n",
        "                    image_file=image,\n",
        "                    log_pattern=captioning_model + \"_\" + calculation_id\n",
        "                ))\n",
        "        except Exception as e:\n",
        "            logger.error(\"Capturing reply error for %s, skipping\",\n",
        "                            parsing_model + \"_\" + calculation_id,\n",
        "                            exc_info=e)\n",
        "            continue\n",
        "\n",
        "    objects = set()\n",
        "    for j in range(0, parsing_iterations):\n",
        "        for i in range(0, capturing_iterations):\n",
        "            try:\n",
        "                llama_reply = llm_infer(\n",
        "                    model=parsing_model,\n",
        "                    prompt=object_extraction_prompt.format(\n",
        "                        llava_reply[i][\"response\"]),\n",
        "                    log_pattern=parsing_model + \"_\" + calculation_id\n",
        "                )\n",
        "                objects = objects | set(text_to_objects(\n",
        "                    text=llama_reply[\"response\"],\n",
        "                    start=start_objects_list_marker,\n",
        "                    end=end_objects_list_marker,\n",
        "                    rem=symbols_to_remove))\n",
        "            except Exception as e:\n",
        "                logger.error(\"Processing reply error for %s, skipping\",\n",
        "                             parsing_model + \"_\" + calculation_id,\n",
        "                             exc_info=e)\n",
        "                continue\n",
        "\n",
        "    image_data = PIL.Image.open(image)\n",
        "\n",
        "    detected_objects = get_model(\n",
        "        name=detection_model).infer(\n",
        "            images=image_data,\n",
        "            prompt=list(objects),\n",
        "            box_threshold=bbox_threshold,\n",
        "            text_threshold=text_threshold\n",
        "        )\n",
        "\n",
        "    return detected_objects\n",
        "\n",
        "\n",
        "def save_detection_results(detection_results: dict[str, any], filename: str) -> None:\n",
        "    data = copy.deepcopy(detection_results)\n",
        "    data[\"boxes\"] = data[\"boxes\"].tolist()\n",
        "    data[\"scores\"] = data[\"scores\"].tolist()\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(json.dumps(data, sort_keys=True, indent=True))\n"
      ],
      "metadata": {
        "id": "9nTA2MFGZ1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Ollama and pulling LLMs"
      ],
      "metadata": {
        "id": "mhX-rdysSQpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ollama.tgz https://ollama.com/download/ollama-linux-amd64.tgz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sqQ48-7MuSKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf ollama.tgz"
      ],
      "metadata": {
        "id": "4PXZ5OOQzujO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T9lBpOTx6Vx"
      },
      "outputs": [],
      "source": [
        "# !curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "# !chmod +x ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.Popen([\"./bin/ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "HEwUusm_zUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bin/ollama pull llava\n",
        "!./bin/ollama pull llama3.1"
      ],
      "metadata": {
        "id": "go9ITRfcWHeR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "result = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "        \"model\": \"llama3.1\",\n",
        "        \"prompt\": \"Why do you cry\",\n",
        "        \"stream\": False\n",
        "    })\n",
        "(result.status_code, result.content)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9IKHdGpNR0kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment #1"
      ],
      "metadata": {
        "id": "FxckiBjz2euJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://github.com/ant-nik/neural_network_course/blob/main/practice_2_data/video_1_fixed/image_001.jpg?raw=true"
      ],
      "metadata": {
        "id": "jzFdqEqE7IUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llava_prompt = \"Describe entities on the image as detailed as possible.\"\n",
        "llama_prompt = \"\"\"Extract all nouns from the TEXT section that are physical objects, living beings, dressing, parts of living beings or physical objects.\n",
        "Split answer in two parts: OUTPUT and INFO.\n",
        "In OUTPUT section place extracted nouns without enumerations symbols and one entity per line.\n",
        "Put detailed explanation of the answer to INFO section.\n",
        "\n",
        "TEXT:\n",
        "{}\n",
        "\n",
        "OUTPUT:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y_XMLwqDUXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "image = PIL.Image.open(\"/content/image.jpg\")\n",
        "detected_objects = detect_all_objects(\n",
        "        image=\"/content/image.jpg\",\n",
        "        captioning_prompt=llava_prompt,\n",
        "        object_extraction_prompt=llama_prompt,\n",
        "        capturing_iterations=2,\n",
        "        parsing_iterations=2\n",
        "        )\n",
        "bbox_image(image=image, bbox=detected_objects)"
      ],
      "metadata": {
        "id": "SkMo2CRt3OU_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_detection_results(detected_objects, \"results.json\")"
      ],
      "metadata": {
        "id": "_NNSPLdi3QBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(load_answer(\"/content/llama3.1_llava_2024-08-23_14.35.15_810177_request.json\")[\"prompt\"])"
      ],
      "metadata": {
        "id": "f1MnaVlDrYl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objects count by confidence score thresholds"
      ],
      "metadata": {
        "id": "ck1zy9IUD4xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = detect_all_objects(\n",
        "        image=\"/content/image.jpg\",\n",
        "        captioning_prompt=llava_prompt,\n",
        "        object_extraction_prompt=llama_prompt,\n",
        "        bbox_threshold=0.01,\n",
        "        text_threshold=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "mzEUUbhWD34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_results[\"class2label\"])"
      ],
      "metadata": {
        "id": "gIT5upgmESNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = numpy.linspace(0.01, 1, 100)\n",
        "y = numpy.diff([len([x for x in filter(lambda x: x > threshold, all_results[\"scores\"])]) for threshold in x])"
      ],
      "metadata": {
        "id": "sdVhFixeEOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express\n",
        "\n",
        "\n",
        "plotly.express.line(x=x[1:], y=y)"
      ],
      "metadata": {
        "id": "2RGWL-5yjUPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotly.express.histogram(y)"
      ],
      "metadata": {
        "id": "6TsKdy2OjYAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.quantile(y, [0.01, 0.05, 0.1, 0.15, 0.2])"
      ],
      "metadata": {
        "id": "wx0W0PIWoB0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = detect_all_objects(\n",
        "        image=\"/content/image.jpg\",\n",
        "        captioning_prompt=llava_prompt,\n",
        "        object_extraction_prompt=llama_prompt,\n",
        "        bbox_threshold=0.08,\n",
        "        text_threshold=0.08\n",
        ")\n",
        "(len(filtered[\"labels\"]), filtered[\"class2label\"])"
      ],
      "metadata": {
        "id": "-xys4gzVgYKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detected entities count vs LLM's iterations"
      ],
      "metadata": {
        "id": "-0QKbPucdtlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llava_attemps = 3\n",
        "llama_attemps = 3\n",
        "averaging_attemps = 3\n",
        "\n",
        "objects_stat = numpy.ndarray(\n",
        "    shape=(llava_attemps, llama_attemps, averaging_attemps, 2),\n",
        "    dtype=float)\n",
        "objects_labels = {}\n",
        "detections = {}\n",
        "for i in range(0, llava_attemps):\n",
        "    objects_labels[i] = {}\n",
        "    detections[i] = {}\n",
        "    for j in range(0, llama_attemps):\n",
        "        objects_labels[i][j] = {}\n",
        "        detections[i][j] = {}\n",
        "        for k in range(0, averaging_attemps):\n",
        "            try:\n",
        "                detected = detect_all_objects(\n",
        "                        image=\"/content/image.jpg\",\n",
        "                        captioning_prompt=llava_prompt,\n",
        "                        object_extraction_prompt=llama_prompt,\n",
        "                        bbox_threshold=0.08,\n",
        "                        text_threshold=0.08,\n",
        "                        capturing_iterations=i + 1,\n",
        "                        parsing_iterations=j + 1\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                objects_stat[i, j, k, 0] = numpy.nan\n",
        "                objects_stat[i, j, k, 1] = numpy.nan\n",
        "                objects_labels[i][j][k] = None\n",
        "                continue\n",
        "\n",
        "            objects_stat[i, j, k, 0] = len(detected[\"class2label\"])\n",
        "            objects_stat[i, j, k, 1] = len(detected[\"labels\"])\n",
        "            objects_labels[i][j][k] = detected[\"labels\"]\n",
        "            detections[i][j][k] = detected"
      ],
      "metadata": {
        "id": "z7xPm4_mebtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"llava_iter\": [], \"llama_iter\": [], \"averaging_index\": [],\n",
        "    \"len(classes)\": [], \"len(labels)\": []\n",
        "}\n",
        "for i in range(0, objects_stat.shape[0]):\n",
        "    for j in range(0, objects_stat.shape[1]):\n",
        "        for k in range(0, objects_stat.shape[2]):\n",
        "            data[\"llava_iter\"].append(i + 1)\n",
        "            data[\"llama_iter\"].append(j + 1)\n",
        "            data[\"averaging_index\"].append(k)\n",
        "            data[\"len(classes)\"].append(objects_stat[i, j, k, 0])\n",
        "            data[\"len(labels)\"].append(objects_stat[i, j, k, 1])\n",
        "\n",
        "dataframe = pandas.DataFrame(data)\n",
        "dataframe.to_csv(\"iterations_stat.csv\")\n",
        "\n",
        "with open(\"full_detection.json\", \"w\") as stat:\n",
        "    stat.write(str(detections))"
      ],
      "metadata": {
        "id": "tDU0ogOYkU2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_classes_arr = numpy.ndarray(\n",
        "    shape=(llava_attemps, llama_attemps), dtype=float\n",
        ")\n",
        "\n",
        "len_labels_arr = numpy.ndarray(\n",
        "    shape=(llava_attemps, llama_attemps), dtype=float\n",
        ")\n",
        "\n",
        "groups = dataframe.groupby(by=[\"llava_iter\", \"llama_iter\"])\n",
        "for index, row in groups.mean().iterrows():\n",
        "    ind = (index[0] - 1, index[1] - 1)\n",
        "    len_classes_arr[ind] = row[\"len(classes)\"]\n",
        "    len_labels_arr[ind] = row[\"len(labels)\"]"
      ],
      "metadata": {
        "id": "mNNcGIUk5GTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects\n",
        "\n",
        "\n",
        "fig = plotly.graph_objects.Figure(\n",
        "    data=[\n",
        "        plotly.graph_objects.Surface(\n",
        "            z=len_classes_arr),\n",
        "        plotly.graph_objects.Surface(\n",
        "            z=len_labels_arr)\n",
        "    ])\n",
        "fig.update_layout(title=\"Mean metrics vs (x=llava, y=llama) iterations\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zTDH__8Zjf_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing duplicated text"
      ],
      "metadata": {
        "id": "9gOwIpN4spB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for"
      ],
      "metadata": {
        "id": "D53E3TfTMkAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wT0N5Gl_sb6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}