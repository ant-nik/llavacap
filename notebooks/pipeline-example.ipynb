{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLjWkJOm7r9/2H0BuVfU3a"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation models for zero-shot detection and segmentation\n",
        "\n",
        "Based on [Ollama](https://github.com/ollama/ollama) project."
      ],
      "metadata": {
        "id": "hc5E2kAe2Gxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Python tools"
      ],
      "metadata": {
        "id": "0AFbNeVJSK8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "\n",
        "class ZShotModel:\n",
        "    def __init__(self, model: str):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model=AutoModelForZeroShotObjectDetection.from_pretrained(\n",
        "            model).to(self.device)\n",
        "        self.processor=AutoProcessor.from_pretrained(model)\n",
        "\n",
        "    def infer(self, images: any,\n",
        "              prompt: str | list[str],\n",
        "              box_threshold: float=0.2,\n",
        "              text_threshold: float=0.2):\n",
        "        # VERY important: text queries need to be lowercased + end with a dot\n",
        "        if isinstance(prompt, list):\n",
        "            objects = [item for item in prompt if not item=='']\n",
        "            text = \" . \".join([f\"{item}\" for item in objects]).lower() + \".\"\n",
        "        elif isinstance(prompt, str):\n",
        "            text = prompt.lower() + \".\"\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Error, prompt with type \\\"{type(prompt)}\\\" is not supported\")\n",
        "        inputs = self.processor(\n",
        "            images=images, text=text, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        results = self.processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            target_sizes=[image.size[::-1]]\n",
        "        )\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "def load_answer(filename: str) -> dict[str, any]:\n",
        "    with open(filename, \"r\") as file:\n",
        "        step2_response = json.loads(file.read())\n",
        "    return step2_response[\"response\"] if \"response\" in step2_response else step2_response\n",
        "\n",
        "\n",
        "def llm_infer(model: str,\n",
        "              prompt: str,\n",
        "              image_file: str | list[str] | None=None,\n",
        "              log_pattern: str | None=None,\n",
        "              url: str = \"http://localhost:11434/api/generate\"\n",
        "              ) -> dict[str, any]:\n",
        "    payload = dict(model=model, prompt=prompt, stream=False)\n",
        "    if image_file is not None:\n",
        "        if isinstance(image_file, str):\n",
        "            image_file = [image_file]\n",
        "        images = []\n",
        "        for image in image_file:\n",
        "            with open(image, \"rb\") as file:\n",
        "                encoded_image = base64.b64encode(file.read()).decode(\"ascii\")\n",
        "            images.append(encoded_image)\n",
        "        payload[\"images\"] = images\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_request.json\", \"w\") as file:\n",
        "            file.write(json.dumps(payload))\n",
        "\n",
        "    reply = requests.post(url, json=payload)\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_reply.json\", \"w\") as file:\n",
        "            file.write(reply.content.decode(\"ascii\"))\n",
        "\n",
        "    return json.loads(reply.content.decode(\"ascii\"))"
      ],
      "metadata": {
        "id": "9nTA2MFGZ1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Ollama and pulling LLMs"
      ],
      "metadata": {
        "id": "mhX-rdysSQpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T9lBpOTx6Vx"
      },
      "outputs": [],
      "source": [
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"./ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "HEwUusm_zUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ollama pull llava\n",
        "!./ollama pull llama3.1"
      ],
      "metadata": {
        "id": "go9ITRfcWHeR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "result = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "        \"model\": \"llama3.1\",\n",
        "        \"prompt\": \"Why do you cry\",\n",
        "        \"stream\": False\n",
        "    })\n",
        "(result.status_code, result.content)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9IKHdGpNR0kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdino = ZShotModel(model=\"IDEA-Research/grounding-dino-base\")"
      ],
      "metadata": {
        "id": "kD4BtmxQZbU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment #1"
      ],
      "metadata": {
        "id": "FxckiBjz2euJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://github.com/ant-nik/neural_network_course/blob/main/practice_2_data/video_1_fixed/image_001.jpg?raw=true"
      ],
      "metadata": {
        "id": "jzFdqEqE7IUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llava_reply = llm_infer(\n",
        "    model=\"llava\",\n",
        "    prompt=\"Describe entities on the image as detailed as possible.\",\n",
        "    image_file=\"/content/image.jpg\",\n",
        "    log_pattern=\"llava\"\n",
        ")\n",
        "print(llava_reply)"
      ],
      "metadata": {
        "id": "coMOlkbe2o2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(load_answer(\"llava_reply.json\"))"
      ],
      "metadata": {
        "id": "cRwKtVfmRAsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_prompt = f\"\"\"Extract all nouns from the TEXT section that are physical objects, living beings, dressing, parts of living beings or physical objects.\n",
        "Split answer in two parts: OUTPUT and INFO.\n",
        "In OUTPUT section place extracted nouns without enumerations symbols and one entity per line.\n",
        "Put detailed explanation of the answer to INFO section.\n",
        "\n",
        "TEXT:\n",
        "{llava_reply}\n",
        "\n",
        "OUTPUT:\n",
        "\"\"\"\n",
        "llama_reply = llm_infer(model=\"llama3.1\",\n",
        "    prompt=llama_prompt,\n",
        "    log_pattern=\"llama\"\n",
        ")\n",
        "print(llama_reply)"
      ],
      "metadata": {
        "id": "y_XMLwqDUXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_reply = load_answer(\"llama_reply.json\")\n",
        "print(llama_reply)"
      ],
      "metadata": {
        "id": "TjcieI3VILcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = list(set(llama_reply.replace(\"*\",\"\").replace(\":\", \"\").split(\"OUTPUT\")[1].split(\"INFO\")[0].split(\"\\n\")))\n",
        "objects.remove('')\n",
        "objects"
      ],
      "metadata": {
        "id": "ucWJkVAgP0GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"image.jpg\")"
      ],
      "metadata": {
        "id": "3-X2PYNUgZaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detected_objects = gdino.infer(images=image, prompt=objects)\n",
        "detected_objects"
      ],
      "metadata": {
        "id": "gXT8tFkoUY29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = detected_objects[0][\"labels\"]\n",
        "unique_classes = list(set(labels))\n",
        "class_to_index_map = {\n",
        "    item: unique_classes.index(item) for item in unique_classes\n",
        "}\n",
        "classes = [class_to_index_map[item] for item in detected_objects[0][\"labels\"]]\n",
        "unique_classes"
      ],
      "metadata": {
        "id": "4eyGAmbfoA9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "SDwmJcs2BIat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "vZQKC1XbhwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision\n",
        "import numpy\n",
        "\n",
        "\n",
        "box_annotator = supervision.BoxAnnotator()\n",
        "label_annotator = supervision.LabelAnnotator()\n",
        "\n",
        "image_boxes = supervision.Detections(\n",
        "    xyxy=detected_objects[0][\"boxes\"].cpu().numpy(),\n",
        "    class_id=numpy.array(classes, dtype=int)\n",
        ")\n",
        "\n",
        "#, 2, 3, 4])#results[0][\"labels\"]\n",
        "\"\"\"\n",
        "labels = [\n",
        "    f\"{class_id} {confidence:0.2f}\"\n",
        "    for confidence, class_id, boxes in results\n",
        "]\n",
        "\"\"\"\n",
        "annotated_frame = box_annotator.annotate(scene=image.copy(),\n",
        "                                         detections=image_boxes) #, labels=labels)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=image_boxes,\n",
        "    labels=labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "Fg-kM7s_iTjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "supervision.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "SkMo2CRt3OU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objects count by confidence score thresholds"
      ],
      "metadata": {
        "id": "ck1zy9IUD4xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = gdino.infer(\n",
        "    images=image,\n",
        "    prompt=objects,\n",
        "    box_threshold=0.1,\n",
        "    text_threshold=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "mzEUUbhWD34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = numpy.linspace(0.01, 1, 100)\n",
        "y = numpy.diff([len([x for x in filter(lambda x: x > threshold, all_results[0][\"scores\"])]) for threshold in x])"
      ],
      "metadata": {
        "id": "sdVhFixeEOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express\n",
        "\n",
        "\n",
        "plotly.express.line(x=x[1:], y=y)"
      ],
      "metadata": {
        "id": "2RGWL-5yjUPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotly.express.histogram(y)"
      ],
      "metadata": {
        "id": "6TsKdy2OjYAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.quantile(y, [0.01, 0.05, 0.1, 0.15, 0.2])"
      ],
      "metadata": {
        "id": "wx0W0PIWoB0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}