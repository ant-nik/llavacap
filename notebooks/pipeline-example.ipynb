{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPp/o/IiGIRDAVUCDIs+DiD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation models for zero-shot detection and segmentation\n",
        "\n",
        "Based on [Ollama](https://github.com/ollama/ollama) project."
      ],
      "metadata": {
        "id": "hc5E2kAe2Gxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Python tools"
      ],
      "metadata": {
        "id": "0AFbNeVJSK8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "vZQKC1XbhwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "SDwmJcs2BIat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import torch\n",
        "import PIL\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "import cv2\n",
        "import supervision\n",
        "import numpy\n",
        "import datetime\n",
        "\n",
        "\n",
        "__detection_models = {}\n",
        "\n",
        "\n",
        "def get_model(name: str):\n",
        "    if not name in __detection_models:\n",
        "        __detection_models[name] = ZShotModel(model=name)\n",
        "    return __detection_models[name]\n",
        "\n",
        "\n",
        "class ZShotModel:\n",
        "    def __init__(self, model: str):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model=AutoModelForZeroShotObjectDetection.from_pretrained(\n",
        "            model).to(self.device)\n",
        "        self.processor=AutoProcessor.from_pretrained(model)\n",
        "\n",
        "    def infer(self, images: PIL.Image.Image | list[PIL.Image.Image],\n",
        "              prompt: str | list[str],\n",
        "              box_threshold: float=0.2,\n",
        "              text_threshold: float=0.2):\n",
        "        # VERY important: text queries need to be lowercased + end with a dot\n",
        "        if isinstance(prompt, list):\n",
        "            objects = [item for item in prompt if not item=='']\n",
        "            text = \" . \".join([f\"{item}\" for item in objects]).lower() + \".\"\n",
        "        elif isinstance(prompt, str):\n",
        "            text = prompt.lower() + \".\"\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Error, prompt with type \\\"{type(prompt)}\\\" is not supported\")\n",
        "        inputs = self.processor(\n",
        "            images=images, text=text, return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        results = self.processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            box_threshold=box_threshold,\n",
        "            text_threshold=text_threshold,\n",
        "            target_sizes=[image.size[::-1]]\n",
        "        )\n",
        "\n",
        "        labels = results[0][\"labels\"]\n",
        "        class2label = list(set(labels))\n",
        "        label2class = {\n",
        "            class2label[i]: i for i in range(0, len(class2label))\n",
        "        }\n",
        "        classes = [label2class[label] for label in labels]\n",
        "        results[0][\"class2map\"] = class2label\n",
        "        results[0][\"label2map\"] = label2class\n",
        "        results[0][\"label_classes\"] = classes\n",
        "\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "def load_answer(filename: str) -> dict[str, any]:\n",
        "    with open(filename, \"r\") as file:\n",
        "        step2_response = json.loads(file.read())\n",
        "    return step2_response[\"response\"] if \"response\" in step2_response else step2_response\n",
        "\n",
        "\n",
        "def llm_infer(model: str,\n",
        "              prompt: str,\n",
        "              image_file: str | list[str] | None=None,\n",
        "              log_pattern: str | None=None,\n",
        "              url: str = \"http://localhost:11434/api/generate\"\n",
        "              ) -> dict[str, any]:\n",
        "    payload = dict(model=model, prompt=prompt, stream=False)\n",
        "    if image_file is not None:\n",
        "        if isinstance(image_file, str):\n",
        "            image_file = [image_file]\n",
        "        images = []\n",
        "        for image in image_file:\n",
        "            with open(image, \"rb\") as file:\n",
        "                encoded_image = base64.b64encode(file.read()).decode(\"ascii\")\n",
        "            images.append(encoded_image)\n",
        "        payload[\"images\"] = images\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_request.json\", \"w\") as file:\n",
        "            file.write(json.dumps(payload))\n",
        "\n",
        "    reply = requests.post(url, json=payload)\n",
        "\n",
        "    if log_pattern is not None:\n",
        "        with open(f\"{log_pattern}_reply.json\", \"w\") as file:\n",
        "            file.write(reply.content.decode(\"ascii\"))\n",
        "\n",
        "    return json.loads(reply.content.decode(\"ascii\"))\n",
        "\n",
        "\n",
        "def text_to_objects(\n",
        "        text: str,\n",
        "        start: str=\"OUTPUT\",\n",
        "        end: str=\"INFO\",\n",
        "        rem: str=\":*\") -> list[str]:\n",
        "    clean_list = text\n",
        "    for char in rem:\n",
        "        clean_list = clean_list.replace(char, '')\n",
        "\n",
        "    objects = list(\n",
        "        set(clean_list.split(start)[1].split(end)[0].split(\"\\n\"))\n",
        "    )\n",
        "    objects.remove('')\n",
        "    return objects\n",
        "\n",
        "\n",
        "def bbox_image(image: PIL.Image.Image, bbox: dict[str, any]) -> None:\n",
        "    box_annotator = supervision.BoxAnnotator()\n",
        "    label_annotator = supervision.LabelAnnotator()\n",
        "\n",
        "    image_boxes = supervision.Detections(\n",
        "        xyxy=bbox[\"boxes\"].cpu().numpy(),\n",
        "        class_id=numpy.array(bbox[\"label_classes\"], dtype=int)\n",
        "    )\n",
        "\n",
        "    #, 2, 3, 4])#results[0][\"labels\"]\n",
        "    \"\"\"\n",
        "    labels = [\n",
        "        f\"{class_id} {confidence:0.2f}\"\n",
        "        for confidence, class_id, boxes in results\n",
        "    ]\n",
        "    \"\"\"\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=image.copy(), detections=image_boxes) #, labels=labels)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=image_boxes,\n",
        "        labels=bbox[\"labels\"]\n",
        "    )\n",
        "    supervision.plot_image(annotated_frame, (16, 16))\n",
        "\n",
        "\n",
        "def detect_all_objects(\n",
        "        image: str,\n",
        "        captioning_prompt: str,\n",
        "        object_extraction_prompt: str,\n",
        "        bbox_threshold: float=0.1,\n",
        "        text_threshold: float=0.1,\n",
        "        captioning_model: str=\"llava\",\n",
        "        parsing_model: str=\"llama3.1\",\n",
        "        detection_model: str=\"IDEA-Research/grounding-dino-base\",\n",
        "        start_objects_list_marker: str=\"OUTPUT\",\n",
        "        end_objects_list_marker: str=\"INFO\",\n",
        "        symbols_to_remove: str=\"*:\",\n",
        "        capturing_iterations: int=1,\n",
        "        parsing_iterations: int=1\n",
        ") -> dict[str, any]:\n",
        "    calculation_id = (\n",
        "        captioning_model + \"_\" + datetime.datetime.utcnow().strftime(\n",
        "            \"%Y-%m-%d_%H.%M.%S_%f\")\n",
        "    )\n",
        "\n",
        "    llava_reply = []\n",
        "    objects = set()\n",
        "    for i in range(0, capturing_iterations):\n",
        "        llava_reply = llm_infer(\n",
        "                model=captioning_model,\n",
        "                prompt=captioning_prompt,\n",
        "                image_file=image,\n",
        "                log_pattern=captioning_model + \"_\" + calculation_id\n",
        "            )\n",
        "        for j in range(0, parsing_iterations):\n",
        "            llama_reply = llm_infer(\n",
        "                model=parsing_model,\n",
        "                prompt=llama_prompt.format(llava_reply[\"response\"]),\n",
        "                log_pattern=parsing_model + \"_\" + calculation_id\n",
        "            )\n",
        "\n",
        "            objects = objects | set(text_to_objects(\n",
        "                text=llama_reply[\"response\"],\n",
        "                start=start_objects_list_marker,\n",
        "                end=end_objects_list_marker,\n",
        "                rem=symbols_to_remove))\n",
        "\n",
        "    image_data = PIL.Image.open(image)\n",
        "\n",
        "    detected_objects = get_model(\n",
        "        name=detection_model).infer(\n",
        "            images=image_data,\n",
        "            prompt=list(objects),\n",
        "            box_threshold=bbox_threshold,\n",
        "            text_threshold=text_threshold\n",
        "        )\n",
        "\n",
        "    return detected_objects\n"
      ],
      "metadata": {
        "id": "9nTA2MFGZ1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pfljht7FVDAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Ollama and pulling LLMs"
      ],
      "metadata": {
        "id": "mhX-rdysSQpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T9lBpOTx6Vx"
      },
      "outputs": [],
      "source": [
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.Popen([\"./ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "HEwUusm_zUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ollama pull llava\n",
        "!./ollama pull llama3.1"
      ],
      "metadata": {
        "id": "go9ITRfcWHeR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "result = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "        \"model\": \"llama3.1\",\n",
        "        \"prompt\": \"Why do you cry\",\n",
        "        \"stream\": False\n",
        "    })\n",
        "(result.status_code, result.content)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9IKHdGpNR0kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment #1"
      ],
      "metadata": {
        "id": "FxckiBjz2euJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://github.com/ant-nik/neural_network_course/blob/main/practice_2_data/video_1_fixed/image_001.jpg?raw=true"
      ],
      "metadata": {
        "id": "jzFdqEqE7IUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llava_prompt = \"Describe entities on the image as detailed as possible.\"\n",
        "llama_prompt = \"\"\"Extract all nouns from the TEXT section that are physical objects, living beings, dressing, parts of living beings or physical objects.\n",
        "Split answer in two parts: OUTPUT and INFO.\n",
        "In OUTPUT section place extracted nouns without enumerations symbols and one entity per line.\n",
        "Put detailed explanation of the answer to INFO section.\n",
        "\n",
        "TEXT:\n",
        "{}\n",
        "\n",
        "OUTPUT:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y_XMLwqDUXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "image = PIL.Image.open(\"/content/image.jpg\")\n",
        "detected_objects = detect_all_objects(\n",
        "        image=\"/content/image.jpg\",\n",
        "        captioning_prompt=llava_prompt,\n",
        "        object_extraction_prompt=llama_prompt)\n",
        "bbox_image(image=image, bbox=detected_objects)"
      ],
      "metadata": {
        "id": "SkMo2CRt3OU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(load_answer(\"/content/llama3.1_llava_2024-08-23_14.35.15_810177_request.json\")[\"prompt\"])"
      ],
      "metadata": {
        "id": "f1MnaVlDrYl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objects count by confidence score thresholds"
      ],
      "metadata": {
        "id": "ck1zy9IUD4xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = detect_all_objects(\n",
        "        image=\"/content/image.jpg\",\n",
        "        captioning_prompt=llava_prompt,\n",
        "        object_extraction_prompt=llama_prompt,\n",
        "        bbox_threshold=0.01,\n",
        "        text_threshold=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "mzEUUbhWD34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = numpy.linspace(0.01, 1, 100)\n",
        "y = numpy.diff([len([x for x in filter(lambda x: x > threshold, all_results[\"scores\"])]) for threshold in x])"
      ],
      "metadata": {
        "id": "sdVhFixeEOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express\n",
        "\n",
        "\n",
        "plotly.express.line(x=x[1:], y=y)"
      ],
      "metadata": {
        "id": "2RGWL-5yjUPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotly.express.histogram(y)"
      ],
      "metadata": {
        "id": "6TsKdy2OjYAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.quantile(y, [0.01, 0.05, 0.1, 0.15, 0.2])"
      ],
      "metadata": {
        "id": "wx0W0PIWoB0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}