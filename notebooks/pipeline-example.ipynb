{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwDqx51UEVso71174keaAb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation models for zero-shot detection and segmentation\n",
        "\n",
        "Based on [Ollama](https://github.com/ollama/ollama) project."
      ],
      "metadata": {
        "id": "hc5E2kAe2Gxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T9lBpOTx6Vx"
      },
      "outputs": [],
      "source": [
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"./ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "HEwUusm_zUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ollama pull llava"
      ],
      "metadata": {
        "id": "go9ITRfcWHeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://github.com/ant-nik/neural_network_course/blob/main/practice_2_data/video_1_fixed/image_001.jpg?raw=true"
      ],
      "metadata": {
        "id": "jzFdqEqE7IUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prompt.txt\n",
        "Find objects on the image.\n",
        "Split answer in two sections a LIST and an EXPLANATION.\n",
        "Put only detected object names as single nouns to the LIST section.\n",
        "Put an explanation of the answer into the EXPLANATION section"
      ],
      "metadata": {
        "id": "0wpuEkm6Kw9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '{ \"model\": \"llava\", \"prompt\": \"'`cat prompt.txt`'\", \"images\": [\"'`base64 -w 0 /content/image.jpg`'\"], \"stream\": false}' > body.json"
      ],
      "metadata": {
        "id": "MijjTNOaBUcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o llava-reply.json http://localhost:11434/api/generate --data-binary \"@body.json\""
      ],
      "metadata": {
        "id": "3CI_pQpD1hnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llama-prompt.txt\n",
        "Extract text between LIST and EXPLANATION sections and consider it as TEXT in the instruction below.\n",
        "Split answer in two parts: OUTPUT and INFO.\n",
        "Remove any enumeration symbols in the TEXT and place only one list entity per line to the OUTPUT section between START and END markers.\n",
        "Put any explanation of the answer to INFO section.\n"
      ],
      "metadata": {
        "id": "oymKK-HjQxLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"llama-prompt.txt\") as prompt_file:\n",
        "    llama_prompt = prompt_file.read()\n",
        "with open(\"llava-reply.json\", \"r\") as llava_file:\n",
        "    llama_prompt += json.loads(llava_file.read())[\"response\"]\n",
        "llama_prompt += \"\\n\\nOUTPUT:\\n\\n\"\n",
        "with open(\"llama_prompt.txt\", \"w\") as llama_prompt_file:\n",
        "    llama_prompt_file.write(llama_prompt)"
      ],
      "metadata": {
        "id": "N0aySXk6ZtY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat llama_prompt.txt"
      ],
      "metadata": {
        "id": "A9CWZ5sadfgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '{ \"model\": \"llama3.1\", \"prompt\": \"'`cat llama_prompt.txt`'\", \"stream\": false}' > llama_request_body.json"
      ],
      "metadata": {
        "id": "_Z_gdkksVnfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat llama_request_body.json"
      ],
      "metadata": {
        "id": "lzWoMFSwfxO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./ollama pull llama3.1"
      ],
      "metadata": {
        "id": "4-jPNPWhf9vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --data-binary \"@llama_request_body.json\" -o llama_reply.json http://localhost:11434/api/generate"
      ],
      "metadata": {
        "id": "wLjmLIhW7xHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat llama_reply.json"
      ],
      "metadata": {
        "id": "Ab9FL1uFf4iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"llama_reply.json\", \"r\") as file:\n",
        "    step2_response = json.loads(file.read())\n",
        "print(step2_response[\"response\"])"
      ],
      "metadata": {
        "id": "9nTA2MFGZ1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = [item for item in step2_response[\"response\"].split(\"START\")[1].split(\"END\")[0].split(\"\\n\") if not item=='']\n",
        "objects"
      ],
      "metadata": {
        "id": "XKHsQG7gfjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "model_id = \"IDEA-Research/grounding-dino-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "image = Image.open(\"image.jpg\")"
      ],
      "metadata": {
        "id": "3-X2PYNUgZaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERY important: text queries need to be lowercased + end with a dot\n",
        "text = \" . \".join([f\"all {item}\" for item in objects]).lower() + '.'\n",
        "print(text)"
      ],
      "metadata": {
        "id": "xsTCl5eLsWtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERY important: text queries need to be lowercased + end with a dot\n",
        "text = \" . \".join([f\"{item}\" for item in objects]).lower() + '.'\n",
        "print(text)"
      ],
      "metadata": {
        "id": "3ZT4UFQIkBNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "results = processor.post_process_grounded_object_detection(\n",
        "    outputs,\n",
        "    inputs.input_ids,\n",
        "    box_threshold=0.2,\n",
        "    text_threshold=0.2,\n",
        "    target_sizes=[image.size[::-1]]\n",
        ")\n",
        "results"
      ],
      "metadata": {
        "id": "qUGbiQKPtEA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "vZQKC1XbhwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = results[0][\"labels\"]\n",
        "unique_classes = list(set(labels))\n",
        "class_to_index_map = {\n",
        "    item: unique_classes.index(item) for item in unique_classes\n",
        "}\n",
        "classes = [class_to_index_map[item] for item in results[0][\"labels\"]]"
      ],
      "metadata": {
        "id": "4eyGAmbfoA9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "fLu81SCK2nUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision\n",
        "import numpy\n",
        "\n",
        "\n",
        "box_annotator = supervision.BoxAnnotator()\n",
        "label_annotator = supervision.LabelAnnotator()\n",
        "\n",
        "image_boxes = supervision.Detections(\n",
        "    xyxy=results[0][\"boxes\"].numpy(),\n",
        "    class_id=numpy.array(classes, dtype=int)\n",
        ")\n",
        "\n",
        "#, 2, 3, 4])#results[0][\"labels\"]\n",
        "\"\"\"\n",
        "labels = [\n",
        "    f\"{class_id} {confidence:0.2f}\"\n",
        "    for confidence, class_id, boxes in results\n",
        "]\n",
        "\"\"\"\n",
        "annotated_frame = box_annotator.annotate(scene=image.copy(),\n",
        "                                         detections=image_boxes) #, labels=labels)\n",
        "annotated_frame = label_annotator.annotate(\n",
        "    scene=annotated_frame,\n",
        "    detections=image_boxes,\n",
        "    labels=labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "Fg-kM7s_iTjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "supervision.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "SkMo2CRt3OU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}